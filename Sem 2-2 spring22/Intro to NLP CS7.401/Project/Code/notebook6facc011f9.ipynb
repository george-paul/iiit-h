{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install gdown","metadata":{"id":"agUUHb24atlz","outputId":"b4c18b62-b72b-4f2c-be3b-b16973bab2a2","execution":{"iopub.status.busy":"2022-05-01T10:40:34.840723Z","iopub.execute_input":"2022-05-01T10:40:34.841037Z","iopub.status.idle":"2022-05-01T10:41:02.05295Z","shell.execute_reply.started":"2022-05-01T10:40:34.840953Z","shell.execute_reply":"2022-05-01T10:41:02.052137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://nlp.stanford.edu/data/glove.840B.300d.zip","metadata":{"execution":{"iopub.status.busy":"2022-05-01T10:41:24.088615Z","iopub.execute_input":"2022-05-01T10:41:24.088886Z","iopub.status.idle":"2022-05-01T10:48:15.137854Z","shell.execute_reply.started":"2022-05-01T10:41:24.088856Z","shell.execute_reply":"2022-05-01T10:48:15.136641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip ./glove.840B.300d.zip","metadata":{"execution":{"iopub.status.busy":"2022-05-01T10:58:46.382173Z","iopub.execute_input":"2022-05-01T10:58:46.382443Z","iopub.status.idle":"2022-05-01T10:59:40.396308Z","shell.execute_reply.started":"2022-05-01T10:58:46.382411Z","shell.execute_reply":"2022-05-01T10:59:40.395524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown 10_9Vua3debTFdXwfV9c2YUEsb1ivP5OP --folder\n!gdown 1-ly6i3ocp4AZwtNYrZNr3p3LZcc3Uji2 --folder","metadata":{"execution":{"iopub.status.busy":"2022-05-01T11:00:17.544122Z","iopub.execute_input":"2022-05-01T11:00:17.544408Z","iopub.status.idle":"2022-05-01T11:01:16.011454Z","shell.execute_reply.started":"2022-05-01T11:00:17.544376Z","shell.execute_reply":"2022-05-01T11:01:16.010661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nfrom functools import reduce\nimport json\nimport os\nimport re\nimport tarfile\nimport tempfile\nimport tensorflow as tf\n\nimport numpy as np\nnp.random.seed(1337)  # for reproducibility\n\n'''\n300D Model - Train / Test (epochs)\n=-=-=\nBatch size = 512\nFixed GloVe\n- 300D SumRNN + Translate + 3 MLP (1.2 million parameters) - 0.8315 / 0.8235 / 0.8249 (22 epochs)\n- 300D GRU + Translate + 3 MLP (1.7 million parameters) - 0.8431 / 0.8303 / 0.8233 (17 epochs)\n- 300D LSTM + Translate + 3 MLP (1.9 million parameters) - 0.8551 / 0.8286 / 0.8229 (23 epochs)\nFollowing Liu et al. 2016, I don't update the GloVe embeddings during training.\nUnlike Liu et al. 2016, I don't initialize out of vocabulary embeddings randomly and instead leave them zeroed.\nThe jokingly named SumRNN (summation of word embeddings) is 10-11x faster than the GRU or LSTM.\nOriginal numbers for sum / LSTM from Bowman et al. '15 and Bowman et al. '16\n=-=-=\n100D Sum + GloVe - 0.793 / 0.753\n100D LSTM + GloVe - 0.848 / 0.776\n300D LSTM + GloVe - 0.839 / 0.806\n'''\n\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import BatchNormalization\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.regularizers import l2\nfrom keras.utils import np_utils","metadata":{"id":"YF_ffyLLrf-I","execution":{"iopub.status.busy":"2022-05-01T11:02:32.75142Z","iopub.execute_input":"2022-05-01T11:02:32.752012Z","iopub.status.idle":"2022-05-01T11:02:37.472514Z","shell.execute_reply.started":"2022-05-01T11:02:32.751973Z","shell.execute_reply":"2022-05-01T11:02:37.471807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def extract_tokens_from_binary_parse(parse):\n#     return parse.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').split()\n\n# def yield_examples(fn, skip_no_majority=True, limit=None):\n#   for i, line in enumerate(open(fn)):\n#     if limit and i > limit:\n#       break\n#     data = json.loads(line)\n#     label = data['gold_label']\n#     s1 = ' '.join(extract_tokens_from_binary_parse(data['sentence1_binary_parse']))\n#     s2 = ' '.join(extract_tokens_from_binary_parse(data['sentence2_binary_parse']))\n#     if skip_no_majority and label == '-':\n#       continue\n#     yield (label, s1, s2)\n\n# def get_data(fn, limit=None):\n#   raw_data = list(yield_examples(fn=fn, limit=limit))\n#   left = [s1 for _, s1, s2 in raw_data]\n#   right = [s2 for _, s1, s2 in raw_data]\n#   print(max(len(x.split()) for x in left))\n#   print(max(len(x.split()) for x in right))\n\n#   LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n#   Y = np.array([LABELS[l] for l, s1, s2 in raw_data])\n#   Y = np_utils.to_categorical(Y, len(LABELS))\n\n#   return left, right, Y\n\n# LPH = Label, Premise, Hypothesis\ndef getLPH(fileName):\n  l = []\n  for _, line in enumerate(open(fileName)):\n    jsonData = json.loads(line)\n    label = jsonData['gold_label']\n    # print(jsonData['sentence2_binary_parse'])\n    getToks = lambda s: ' '.join(s.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').lower().split())\n    p = \"<s> \" + getToks(jsonData['sentence1_binary_parse'])\n    h = \"<s> \" + getToks(jsonData['sentence2_binary_parse'])\n    # print(h)\n\n    # no majority among turks\n    if label == '-':\n      continue\n\n    l.append((label, p, h))\n  return l\n\ndef getData(fileName):\n  data = getLPH(fileName=fileName)\n  Ps = [x[1] for x in data]\n  Hs = [x[2] for x in data]\n  # print(max(len(x.split()) for x in Ps))\n  # print(max(len(x.split()) for x in Hs))\n\n  LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n  Y = np.array([LABELS[x[0]] for x in data])\n  Y = np_utils.to_categorical(Y, len(LABELS))\n  \n  return Ps, Hs, Y","metadata":{"id":"SMW81tIcrkXd","execution":{"iopub.status.busy":"2022-05-01T11:02:57.731883Z","iopub.execute_input":"2022-05-01T11:02:57.732144Z","iopub.status.idle":"2022-05-01T11:02:57.7427Z","shell.execute_reply.started":"2022-05-01T11:02:57.732115Z","shell.execute_reply":"2022-05-01T11:02:57.741669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#RNN = recurrent.LSTM\n#RNN = lambda *args, **kwargs: Bidirectional(recurrent.LSTM(*args, **kwargs))\n#RNN = recurrent.GRU\n#RNN = lambda *args, **kwargs: Bidirectional(recurrent.GRU(*args, **kwargs))\n\n# Summation of word embeddings\nRNN = None\nLAYERS = 1\nUSE_GLOVE = True\nTRAIN_EMBED = False\nEMBED_HIDDEN_SIZE = 300\nSENT_HIDDEN_SIZE = 300\nBATCH_SIZE = 1024\nPATIENCE = 4\nMAX_EPOCHS = 10\nMAX_LEN = 42\nDP = 0.2\nL2 = 4e-6\nACTIVATION = 'relu'\nOPTIMIZER = 'rmsprop'\nprint('RNN / Embed / Sent = {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE))\nprint('GloVe / Trainable Word Embeddings = {}, {}'.format(USE_GLOVE, TRAIN_EMBED))","metadata":{"id":"Fw-QOu2eru7I","execution":{"iopub.status.busy":"2022-05-01T11:03:03.644224Z","iopub.execute_input":"2022-05-01T11:03:03.644837Z","iopub.status.idle":"2022-05-01T11:03:03.651942Z","shell.execute_reply.started":"2022-05-01T11:03:03.644797Z","shell.execute_reply":"2022-05-01T11:03:03.65095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training = get_data('/kaggle/working/snli_1.0_train.jsonl')\n# validation = get_data('/kaggle/working/snli_1.0_dev.jsonl')\n# test = get_data('/kaggle/working/snli_1.0_test.jsonl')\n\n# tokenizer = Tokenizer(lower=False, filters='')\n# tokenizer.fit_on_texts(training[0] + training[1])\n\ntraining = getData('/content/drive/MyDrive/nli-datasets/snli/snli_1.0/snli_1.0_train.jsonl')\nvalidation = getData('/content/drive/MyDrive/nli-datasets/snli/snli_1.0/snli_1.0_dev.jsonl')\ntest = getData('/content/drive/MyDrive/nli-datasets/snli/snli_1.0/snli_1.0_test.jsonl')\n\ntokenizer = Tokenizer(filters = '!\"#$%&()*+,-/:;=?@[\\\\]^_`{|}~\\t\\n', oov_token=\"<unk>\")\ntokenizer.fit_on_texts(training[0] + training[1])\n\n# Lowest index from the tokenizer is 1 - we need to include 0 in our vocab count\nVOCAB = len(tokenizer.word_counts) + 1\nLABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n\n\n\nto_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\nprepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n\ntraining = prepare_data(training)\nvalidation = prepare_data(validation)\ntest = prepare_data(test)","metadata":{"id":"kxMb9Vq3ZQBJ","outputId":"850f1648-73fe-4d92-ae96-25d2634e762a","execution":{"iopub.status.busy":"2022-05-01T05:35:06.251576Z","iopub.execute_input":"2022-05-01T05:35:06.251841Z","iopub.status.idle":"2022-05-01T05:35:43.974575Z","shell.execute_reply.started":"2022-05-01T05:35:06.251811Z","shell.execute_reply":"2022-05-01T05:35:43.973828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training","metadata":{"id":"ZljNX1FmatrI","outputId":"714b49a9-3c44-4487-a31e-b3c6415f0839"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nprint('Build model...')\nprint('Vocab size =', VOCAB)\n\nGLOVE_STORE = 'precomputed_glove.weights'\nif USE_GLOVE:\n  if not os.path.exists(GLOVE_STORE + '.npy'):\n    print('Computing GloVe')\n  \n    embeddings_index = {}\n    f = open('./glove.840B.300d.txt')\n    for line in f:\n      values = line.split(' ')\n      word = values[0]\n      coefs = np.asarray(values[1:], dtype='float32')\n      embeddings_index[word] = coefs\n    f.close()\n    \n    # prepare embedding matrix\n    embedding_matrix = np.zeros((VOCAB, EMBED_HIDDEN_SIZE))\n    for word, i in tokenizer.word_index.items():\n      embedding_vector = embeddings_index.get(word)\n      if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n      else:\n        print('Missing from GloVe: {}'.format(word))\n  \n    np.save(GLOVE_STORE, embedding_matrix)\n\n  print('Loading GloVe')\n  embedding_matrix = np.load(GLOVE_STORE + '.npy')\n\n  print('Total number of null word embeddings:')\n  print(np.sum(np.sum(embedding_matrix, axis=1) == 0))\n\n  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=TRAIN_EMBED)\nelse:\n  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)\n\nrnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout_W=DP, dropout_U=DP)\nSumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n\ntranslate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n\npremise = Input(shape=(MAX_LEN,), dtype='int32')\nhypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n\nprem = embed(premise)\nhypo = embed(hypothesis)\n\nprem = translate(prem)\nhypo = translate(hypo)\n\nif RNN and LAYERS > 1:\n  for l in range(LAYERS - 1):\n    rnn = RNN(return_sequences=True, **rnn_kwargs)\n    prem = BatchNormalization()(rnn(prem))\n    hypo = BatchNormalization()(rnn(hypo))\nrnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\nprem = rnn(prem)\nhypo = rnn(hypo)\nprem = BatchNormalization()(prem)\nhypo = BatchNormalization()(hypo)\n\n","metadata":{"id":"UNCH_a60UiGy","execution":{"iopub.status.busy":"2022-05-01T06:01:29.833072Z","iopub.execute_input":"2022-05-01T06:01:29.833876Z","iopub.status.idle":"2022-05-01T06:04:13.369614Z","shell.execute_reply.started":"2022-05-01T06:01:29.833837Z","shell.execute_reply":"2022-05-01T06:04:13.367858Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# seq2seq using GloVe","metadata":{}},{"cell_type":"code","source":"joint = keras.layers.concatenate([prem, hypo])\njoint = Dropout(DP)(joint)\nfor i in range(3):\n  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n  joint = Dropout(DP)(joint)\n  joint = BatchNormalization()(joint)\n\npred = Dense(len(LABELS), activation='softmax')(joint)\n\nmodel = Model(inputs=[premise, hypothesis], outputs=pred)\nmodel.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n\nprint('Training')\n_, tmpfn = tempfile.mkstemp()\n# Save the best model during validation and bail out of training early if we're not improving\ncallbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]\nmodel.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n\n# Restore the best found model during validation\nmodel.load_weights(tmpfn)\n\nloss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\nprint('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))","metadata":{"id":"MJjZtKJufpY1","outputId":"74a87913-04fd-4f7e-b686-2f33db150b7a","execution":{"iopub.status.busy":"2022-05-01T06:10:17.71981Z","iopub.execute_input":"2022-05-01T06:10:17.720138Z","iopub.status.idle":"2022-05-01T06:16:35.55359Z","shell.execute_reply.started":"2022-05-01T06:10:17.720086Z","shell.execute_reply":"2022-05-01T06:16:35.552159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN - LSTM","metadata":{"id":"qRv-U9ivw4NC"}},{"cell_type":"code","source":"LAYERS = 1\nRNN = recurrent.LSTM\n# RNN = lambda *args, **kwargs: Bidirectional(recurrent.LSTM(*args, **kwargs))\n#RNN = recurrent.GRU\n#RNN = lambda *args, **kwargs: Bidirectional(recurrent.GRU(*args, **kwargs))\n","metadata":{"id":"4RB8ITdewyVC","execution":{"iopub.status.busy":"2022-05-01T06:16:41.937667Z","iopub.execute_input":"2022-05-01T06:16:41.93793Z","iopub.status.idle":"2022-05-01T06:16:41.942115Z","shell.execute_reply.started":"2022-05-01T06:16:41.937899Z","shell.execute_reply":"2022-05-01T06:16:41.941209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\n# SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n\npremise = Input(shape=(MAX_LEN,), dtype='int32')\nhypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n\nprem = embed(premise)\nhypo = embed(hypothesis)\n\nprem = translate(prem)\nhypo = translate(hypo)","metadata":{"id":"8du2hBUc1mu0","execution":{"iopub.status.busy":"2022-05-01T06:16:44.369316Z","iopub.execute_input":"2022-05-01T06:16:44.370112Z","iopub.status.idle":"2022-05-01T06:16:44.400833Z","shell.execute_reply.started":"2022-05-01T06:16:44.37006Z","shell.execute_reply":"2022-05-01T06:16:44.400171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RNN and LAYERS > 1:\n  for l in range(LAYERS - 1):\n    rnn = RNN(return_sequences=True, **rnn_kwargs)\n    prem = BatchNormalization()(rnn(prem))\n    hypo = BatchNormalization()(rnn(hypo))\nrnn = RNN(return_sequences=False, **rnn_kwargs)\nprem = rnn(prem)\nhypo = rnn(hypo)\nprem = BatchNormalization()(prem)\nhypo = BatchNormalization()(hypo)","metadata":{"id":"zYCZKsi6wxyH","execution":{"iopub.status.busy":"2022-05-01T06:16:48.325843Z","iopub.execute_input":"2022-05-01T06:16:48.326109Z","iopub.status.idle":"2022-05-01T06:16:48.594404Z","shell.execute_reply.started":"2022-05-01T06:16:48.326079Z","shell.execute_reply":"2022-05-01T06:16:48.593677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joint = keras.layers.concatenate([prem, hypo])\njoint = Dropout(DP)(joint)\nfor i in range(3):\n  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n  joint = Dropout(DP)(joint)\n  joint = BatchNormalization()(joint)\n\npred = Dense(len(LABELS), activation='softmax')(joint)\n\nmodel = Model(inputs=[premise, hypothesis], outputs=pred)\nmodel.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n\nprint('Training')\n_, tmpfn = tempfile.mkstemp()\n# Save the best model during validation and bail out of training early if we're not improving\ncallbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/gdrive/MyDrive/GCDC_rerelease/checkpoints', save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)\nmodel.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n\n# Restore the best found model during validation\nmodel.load_weights(tmpfn)\n\nloss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\nprint('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))","metadata":{"id":"cN5hdhLPw_Nx","outputId":"1f52d79d-cd89-45b5-9d61-e0a7c67cd3d8","execution":{"iopub.status.busy":"2022-05-01T06:16:51.70071Z","iopub.execute_input":"2022-05-01T06:16:51.700975Z"},"trusted":true},"execution_count":null,"outputs":[]}]}