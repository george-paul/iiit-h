{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Project-Final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessory modules"
      ],
      "metadata": {
        "id": "d9uq612NK3aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(69)\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from __future__ import print_function\n",
        "from functools import reduce\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "g5g1SqgqKmYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing data"
      ],
      "metadata": {
        "id": "ncPom3KBLiFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LPH = Label, Premise, Hypothesis\n",
        "def getLPH(fileName):\n",
        "  l = []\n",
        "  for _, line in enumerate(open(fileName)):\n",
        "    jsonData = json.loads(line)\n",
        "    label = jsonData['gold_label']\n",
        "    getToks = lambda s: ' '.join(s.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').lower().split())\n",
        "    p = \"<s> \" + getToks(jsonData['sentence1_binary_parse'])\n",
        "    h = \"<s> \" + getToks(jsonData['sentence2_binary_parse'])\n",
        "    \n",
        "    if label == '-':\n",
        "      continue\n",
        "\n",
        "    l.append((label, p, h))\n",
        "  return l\n",
        "\n",
        "def getData(fileName):\n",
        "  data = getLPH(fileName=fileName)\n",
        "  Ps = [x[1] for x in data]\n",
        "  Hs = [x[2] for x in data]\n",
        "\n",
        "  LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
        "  Y = np.array([LABELS[x[0]] for x in data])\n",
        "  Y = np_utils.to_categorical(Y, len(LABELS))\n",
        "  \n",
        "  return Ps, Hs, Y"
      ],
      "metadata": {
        "id": "WgyN5elALkSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Settings\n"
      ],
      "metadata": {
        "id": "i175EBDUL7cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RNN = None\n",
        "LAYERS = 1\n",
        "USE_GLOVE = True\n",
        "TRAIN_EMBED = False\n",
        "EMBED_HIDDEN_SIZE = 300\n",
        "SENT_HIDDEN_SIZE = 300\n",
        "BATCH_SIZE = 512\n",
        "PATIENCE = 4\n",
        "MAX_EPOCHS = 20\n",
        "MAX_LEN = 42\n",
        "DP = 0.2\n",
        "L2 = 4e-6\n",
        "ACTIVATION = 'relu'\n",
        "OPTIMIZER = 'rmsprop'"
      ],
      "metadata": {
        "id": "dR9p7N5lL9sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading SNLI dataset"
      ],
      "metadata": {
        "id": "6byOGEuBMHuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training = get_data('/content/drive/MyDrive/nli-datasets/snli/snli_1.0/snli_1.0_train.jsonl')\n",
        "validation = get_data('/content/drive/MyDrive/nli-datasets/snli/snli_1.0/snli_1.0_dev.jsonl')\n",
        "test = get_data('/content/drive/MyDrive/nli-datasets/snli/snli_1.0/snli_1.0_test.jsonl')\n",
        "\n",
        "tokenizer = Tokenizer(lower=False, filters='')\n",
        "tokenizer.fit_on_texts(training[0] + training[1])\n",
        "\n",
        "VOCAB = len(tokenizer.word_counts) + 1\n",
        "LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
        "\n",
        "to_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\n",
        "prepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n",
        "\n",
        "training = prepare_data(training)\n",
        "validation = prepare_data(validation)\n",
        "test = prepare_data(test)"
      ],
      "metadata": {
        "id": "Rq5XePQVMMBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading MultiNLI dataset"
      ],
      "metadata": {
        "id": "JSCic91LN6Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training = getData('/content/drive/MyDrive/nli-datasets/multinli/multinli_1.0/multinli_1.0_train.jsonl')\n",
        "validation = getData('/content/drive/MyDrive/nli-datasets/multinli/multinli_1.0/multinli_1.0_dev_matched.jsonl')\n",
        "test = getData('/content/drive/MyDrive/nli-datasets/multinli/multinli_1.0/multinli_1.0_dev_mismatched.jsonl')\n",
        "\n",
        "tokenizer = Tokenizer(filters = '!\"#$%&()*+,-/:;=?@[\\\\]^_`{|}~\\t\\n', oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(training[0] + training[1])\n",
        "\n",
        "VOCAB = len(tokenizer.word_counts) + 2\n",
        "LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
        "\n",
        "to_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\n",
        "prepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n",
        "\n",
        "training = prepare_data(training)\n",
        "validation = prepare_data(validation)\n",
        "test = prepare_data(test)"
      ],
      "metadata": {
        "id": "hSm9qaTPN52R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GloVe Embeddings"
      ],
      "metadata": {
        "id": "5wXmkaToOKPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GLOVE_STORE = 'Link to pre existing embeddings if present'\n",
        "if USE_GLOVE:\n",
        "  if not os.path.exists(GLOVE_STORE + '.npy'):\n",
        "    print('Computing GloVe')\n",
        "  \n",
        "    embeddings_index = {}\n",
        "    f = open('/content/drive/MyDrive/nli-datasets/GloVe-Embeddings/glove.840B.300d.txt')\n",
        "    for line in f:\n",
        "      values = line.split(' ')\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    \n",
        "    # prepare embedding matrix\n",
        "    embedding_matrix = np.zeros((VOCAB, EMBED_HIDDEN_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "      else:\n",
        "        print('Missing from GloVe: {}'.format(word))\n",
        "  \n",
        "    np.save(GLOVE_STORE, embedding_matrix)\n",
        "\n",
        "  print('Loading GloVe')\n",
        "  embedding_matrix = np.load(GLOVE_STORE + '.npy')\n",
        "\n",
        "  print('Total number of null word embeddings:')\n",
        "  print(np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=TRAIN_EMBED)\n",
        "else:\n",
        "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)"
      ],
      "metadata": {
        "id": "uBv3dOiCONVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "jcqtEfkVPY2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN, SNLI, Sigmoid\n",
        "`Change ACTIVATION to sigmoid in Settings`"
      ],
      "metadata": {
        "id": "aETbHznbQ0YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\n",
        "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
        "\n",
        "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)\n",
        "\n",
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)"
      ],
      "metadata": {
        "id": "C99hGqVQRDRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint = keras.layers.concatenate([prem, hypo])\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='sigmoid')(joint)\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "fn='/content/drive/MyDrive/bestvalid'\n",
        "\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(filepath=fn, save_best_only=True, save_weights_only=True)]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(fn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "metadata": {
        "id": "XKrUIehpRR1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN, SNLI, ReLU\n",
        "`No change in settings needed`"
      ],
      "metadata": {
        "id": "KWjTEJXORaRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\n",
        "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
        "\n",
        "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)\n",
        "\n",
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)"
      ],
      "metadata": {
        "id": "ejs-iGCYRjZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint = keras.layers.concatenate([prem, hypo])\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "fn='/content/drive/MyDrive/bestvalid'\n",
        "\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(filepath=fn, save_best_only=True, save_weights_only=True)]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(fn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "metadata": {
        "id": "3QGZ-vVFR4Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN, SNLI, Softmax\n",
        "`Change ACTIVATION to softmax in Settings`"
      ],
      "metadata": {
        "id": "E74OGnJ9SEfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\n",
        "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
        "\n",
        "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)\n",
        "\n",
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)"
      ],
      "metadata": {
        "id": "iaBr0kBdSUvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint = keras.layers.concatenate([prem, hypo])\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "fn='/content/drive/MyDrive/bestvalid'\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(filepath=fn, save_best_only=True, save_weights_only=True)]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(fn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "metadata": {
        "id": "IWbDZa3SSeHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN, MultiNLI, Sigmoid\n",
        "`Change ACTIVATION to sigmoid in Settings`"
      ],
      "metadata": {
        "id": "ldvbDUrASiqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\n",
        "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
        "\n",
        "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)\n",
        "\n",
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)"
      ],
      "metadata": {
        "id": "GHk85K-kS0Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint = keras.layers.concatenate([prem, hypo])\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='sigmoid')(joint)\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "fn='/content/drive/MyDrive/bestvalid'\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(filepath=fn, save_best_only=True, save_weights_only=True)]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(fn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "metadata": {
        "id": "LbjxeiM5S9uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN, MultiNLI, ReLU\n",
        "`No change in Settings needed`"
      ],
      "metadata": {
        "id": "NtO7OEgJTA-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\n",
        "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
        "\n",
        "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)\n",
        "\n",
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)"
      ],
      "metadata": {
        "id": "rLwzWmPSTKFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint = keras.layers.concatenate([prem, hypo])\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='relu')(joint)\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "fn='/content/drive/MyDrive/bestvalid'\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(filepath=fn, save_best_only=True, save_weights_only=True)]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(fn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "metadata": {
        "id": "geAeQR0TTNWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RNN, MultiNLI, Softmax\n",
        "`Change ACTIVATION to softmax in Settings`"
      ],
      "metadata": {
        "id": "zh-E0cN4TPwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\n",
        "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
        "\n",
        "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)\n",
        "\n",
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)"
      ],
      "metadata": {
        "id": "pQK6pVU2TXvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint = keras.layers.concatenate([prem, hypo])\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "print('Training')\n",
        "fn='/content/drive/MyDrive/bestvalid'\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(filepath=fn, save_best_only=True, save_weights_only=True)]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(fn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "metadata": {
        "id": "VyafAheJTakc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM, SNLI\n",
        "`No change in Settings needed`"
      ],
      "metadata": {
        "id": "Yu0Gjw4MTrqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LAYERS = 1\n",
        "RNN = recurrent.LSTM\n",
        "BATCH_SIZE = 1024\n",
        "PATIENCE = 4\n",
        "MAX_EPOCHS = 10"
      ],
      "metadata": {
        "id": "PzFgR-NcTv03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)"
      ],
      "metadata": {
        "id": "EFVrFIvoT-Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)"
      ],
      "metadata": {
        "id": "0QgBHDtZUEk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint = keras.layers.concatenate([prem, hypo])\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "fn = '/content/drive/MyDrive/checkpoints/LSTM'\n",
        "\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(filepath=fn, save_best_only=True, save_weights_only=True, monitor='val_accuracy')]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
        "model.save(\"/content/drive/MyDrive/checkpoints/LSTMmodel.h5\")\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(fn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "metadata": {
        "id": "QLeFD2ilUKkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU, SNLI"
      ],
      "metadata": {
        "id": "eHElKg6gWvCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LAYERS = 1\n",
        "RNN = recurrent.GRU"
      ],
      "metadata": {
        "id": "6lmMAzL8WuTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_kwargs = dict(units=SENT_HIDDEN_SIZE, dropout=DP, recurrent_dropout=DP)\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)"
      ],
      "metadata": {
        "id": "7xmOvoTkW1K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)"
      ],
      "metadata": {
        "id": "Nz_f9nuzW4PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint = keras.layers.concatenate([prem, hypo])\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, kernel_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
        "\n",
        "model = Model(inputs=[premise, hypothesis], outputs=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "fn=\"./checkpoints\"\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(filepath=fn, save_weights_only=True, monitor='val_accuracy', mode='max', save_best_only=True)]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_data=([validation[0], validation[1]], validation[2]), callbacks=callbacks)\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(fn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "metadata": {
        "id": "q549NUBKXFwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualising with confusion matrix and heat map. Calculation of F-Score, Precision and Recall."
      ],
      "metadata": {
        "id": "qeEsp2KNUl0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis_statistics(model, test_data, y_test, accuracy):\n",
        "  test_pred = model.predict(test_data)\n",
        "\n",
        "  encoded_list_y_test = []\n",
        "  for val in y_test:\n",
        "    idx = np.argmax(val)\n",
        "    encoded_list_y_test.append(idx+1)\n",
        "  encoded_list_y_test\n",
        "  \n",
        "  encoded_list_test_pred = []\n",
        "  for val in test_pred:\n",
        "    idx = np.argmax(val)\n",
        "    encoded_list_test_pred.append(idx+1)\n",
        "  encoded_list_test_pred\n",
        "\n",
        "  confusion_1 = metrics.confusion_matrix(encoded_list_y_test,encoded_list_test_pred)\n",
        "\n",
        "  plt.figure(figsize=(9,9))\n",
        "  sns.heatmap(confusion_1, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues');\n",
        "  plt.ylabel('Actual label');\n",
        "  plt.xlabel('Predicted label');\n",
        "  all_sample_title = 'Accuracy Score: {0:.3g}'.format(accuracy)\n",
        "  plt.title(all_sample_title, size = 15);\n",
        "  \n",
        "  print(classification_report(encoded_list_y_test, encoded_list_test_pred))"
      ],
      "metadata": {
        "id": "U8zMzGsuUy-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_statistics(model, [test[0],test[1]], test[2], acc)"
      ],
      "metadata": {
        "id": "OaWR0SRSVTBf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}