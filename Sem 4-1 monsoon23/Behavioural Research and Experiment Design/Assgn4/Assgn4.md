# BRED - Assignment 4

<div align=right><b>George Paul</b></div>

<div align=right><b>Balaji Venkateswarar</b></div>



## Part 1

The selected presentation was: 

> A study on Evaluation Effectiveness at IIIT-H
>
> By Nikhil Chandak and Shashwat Goel

The project was similar our project since it measures the effectiveness of two methods of evaluation of the same thing. There is also a similarity since it measures a digital evaluation vs pen-and-paper evaluation. Although only some of the online methods of evaluation are meant to emulate their offline counterparts. For example, exams are often just conducted within open-book circumstances as opposed to trying to enact effective proctoring.



The research question is certainly an important one, especially in mid- and post-COVID times where almost everything tried to move to an online mode including evaluations.

Operationalization was mostly done through items on a survey. Employing Likert scales where necessary and asking qualitative questions when quantitative ones won't really be helpful and vice versa.

Hypothesis formulation was done well.

Sampling was probably done via volunteer sampling as is the case for most studies in IIIT. Although it ends up being a helpful method for this particular research question considering most students are the target demographic.

The variables are well-defined and attempts were made to control for extraneous variables. Any that weren't controlled are mentioned in the presentation.

A very thorough data analysis was done with good uses of the standard measures such as the mean, variance etc as well as some more niche tests like the Levene's Test and the Spearman Correlation.



Due to many limitations the experiment design doesn't address many alternative evaluation modes such as projects. Those that were addressed were sometimes not effectively operationalized such as those that directly asked participants about what they _think_ their means are versus just getting their means. This was perhaps out of the scope of the study and they do mention that they were unable to procure the proper data through no fault of their own.

## Part 2

Our own experiment was:

> Testing the validity of a developed digital ICMR-NCTB

The Neuro Cognitive Tool Box (NCTB) is a set of cognitive tests that have been previously validated and standardised for the pen-and-paper medium. A digital version was developed by the team and the project we took up was to make an attempt at validating the tests in the battery.

Due to mostly the limited scope of the project many of the variables could not be operationalized. This lead to a lacking review of the entire battery but we decided instead to focus on two of tests present in the batteries. The operationalization was still lacking since we couldn't acquire the corresponding data for each participant's performance on the pen-and-paper version of the tests. As a stand-in we resorted to comparing with the established norms of the pen-and-paper tests set by previous work. Although matching these scores is important, we have no method to control for variables such as all the samples being from engineering university students. That, also of note, all knew how to use a tablet device efficiently. Although the sample size wasn't necessarily small, the number of data points collected did not enable us enough to do a more thorough statistical analysis.

To work around this, possible future work can be to operationalize data points such as the participants' performance on the pen-and-paper tasks. With this sort of data, the statistical analysis can be extended as well to include perhaps some reliability and correlation tests.