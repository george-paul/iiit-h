{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3622bff",
   "metadata": {},
   "source": [
    "# Assignment 1 - Text Retrieval  (23/08/2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366cb29",
   "metadata": {},
   "source": [
    "## 1. Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb979c5",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    \n",
    "In this lab, we introduce two basic models in information retrieval, namely, the <b><i> boolean </i></b> and <b><i>vector models</i></b>. The following tasks should help you understand better how to find a discriminative set of <b><i> index terms </i></b>(keywords) for collection of documents. We study how to represent each document and query as a vector.\n",
    "    \n",
    "In order to do this, we perform stemming by applying the <b><i>Porter algorithm</i></b> and visualize frequency of words using the <b><i>tag clouds </i></b> over the stems already obtained.\n",
    "    \n",
    "Next, we extract the most informative words from documents based on the <b><i>term frequency-inverse documents frequency </i></b>(tf-idf) weighting scheme.\n",
    "    Then, we complete the <b><i>boolean</i></b> and <b><i>vector models</i></b> to predict that the certain documents are relevant to the particular query.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed0c2f",
   "metadata": {},
   "source": [
    "## 2. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e2f3e",
   "metadata": {},
   "source": [
    "### 2.1 Useful terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d649ab",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<b><i>Terms</b></i> is a set of words on which the vector representation is based. It is also referred as <b><i>index</b></i> or <b><i>index set</b></i>. Examples of terms are \"computer\", \"software\" and \"fishing\".\n",
    "\n",
    "<b><i>Term weight</b></i> is a scalar parameter that represents the significance of a given term in a given document. \n",
    "\n",
    "<b><i>Term document matrix</b></i> is a matrix consisting of all vector representations of the document in a corpus. By convention, rows correspond to terms and columns correspond to documents. \n",
    "\n",
    "<b><i>Document ranking</b></i> is ranking of documents based on the similarity to a certain query.\n",
    "\n",
    "<b><i>Frequency</b></i> is number of occurrences of a word in a text.\n",
    "\n",
    "<b><i>Corpus</b></i> is a collection of documents.\n",
    "\n",
    "<b><i>Rank of a word</b></i> is a word's ordinal number in a list sorted by decreasing frequency.\n",
    "\n",
    "<b><i>Vocabulary</b></i> is a set of all unique words in a corpus.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d68bfe",
   "metadata": {},
   "source": [
    "### 2.2 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e3858e",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    \n",
    "A <b><i>stem</b></i> is the part of a word to which affixes (suffixes and prefixes) can be attached to form new\n",
    "words. A few examples of words that have the same stem are given below.\n",
    "- 'house', 'houses', 'housing'\n",
    "- 'set', 'sets', 'reset', 'setting', 'setters'\n",
    "- 'tall', 'taller', 'tallest'\n",
    "- 'steam', 'steamers', 'steamier', 'steaming'\n",
    "    \n",
    "<b><i>Stemming</b></i> is the process of extracting for each given word its corresponding stem. Several popular implementations for the English language can be found <a href=\"http://tartarus.org/~martin/PorterStemmer\" target=\"_blank\">here</a>.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd5adc",
   "metadata": {},
   "source": [
    "### 2.3 Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d452a",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "    \n",
    "<b><i>Stop words</b></i> are those words which are judged to be so common that they have very little information or discrimination power. For the French language, de, et, la, que, vous, etc. are examples of <b><i>stop words</b></i>. Such words are usually useless for information retrieval. Thus, in order to enhance system performance, stop words can be left out of consideration. In online systems they are\n",
    "not indexed, and therefore are not searchable. A list of English <b><i>stop words</b></i> is given in the provided file <b>english.stop</b>.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba27ec",
   "metadata": {},
   "source": [
    "### 2.4 Tag clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfce033",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<b><i>Tag clouds</b></i> are visual representation of words (tags) in documents (websites). Tags will be represented by different font size based on the importance of each word (frequency). More important words appear larger in the visual representation. Generally tags are listed in alphabetical order.\n",
    "The importance of a word is given by its popularity (frequency).\n",
    "To compute the font size $s_{i}$ for a word $i$, we will use the formula\n",
    "\\begin{equation}\n",
    "s_{i}= f_{max} \\cdot \\frac{t_{i}-t_{min}}{t_{max}-t_{min}},\n",
    "\\end{equation}\n",
    "where $f_{max}$ is the maximum font size, $t_{i}$ is the word count, $t_{min}$ is the minimum count, and $t_{max}$ is the maximum count.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f101fc",
   "metadata": {},
   "source": [
    "### 2.5 Term Frequency-Inverse Documents Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97efd7d",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "The <b><i>tf-idf</b></i> is a weight used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is for a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word\n",
    "in the corpus. Variations of the <b><i>tf-idf</b></i>  weighting scheme are often used by search engines as a central tool in scoring and ranking a documents relevance given a user query.\n",
    "The <b><i>term frequency</b></i> $tf_{ij}$ is the weight of a term $t_{i}$ in a document $d_{j}$ computed according to\n",
    "\\begin{equation}\n",
    "tf_{ij}= \\frac{freq_{ij}}{\\max\\limits_{k} freq_{kj}},\n",
    "\\end{equation}\n",
    "where $freq_{ij}$ is the number of occurrences of the term $t_{i}$ in the document $d_{j}$. As a result, the most frequent term $t_{\\bar{i}}$\n",
    "in a document $d_{j}$ always has $tf_{\\bar{i}j} = 1$.\n",
    "    \n",
    "The <b><i>inverse document frequency</b></i> $idf_{i}$ is a measure of the general importance of the term $t_{i}$ in\n",
    "the whole corpus. It is obtained by dividing the number of all documents by the number of\n",
    "documents containing the term  $t_{i}$, and then taking the logarithm of that quotient as\n",
    "\\begin{equation}\n",
    "idf_{i}= \\log \\frac{N}{n_{i}},\n",
    "\\end{equation}\n",
    "where $N$ is the total number of documents, and $n_{i}$ is the number of documents in which the\n",
    "term $t_{i}$ appears.\n",
    "Using the <b><i>term frequency</b></i> $tf_{ij}$ and the <b><i>inverse document frequency</b></i>  $idf_{i}$ as described above, we obtain the most used term-weighting scheme in text\n",
    "retrieval defned as\n",
    "\\begin{equation}\n",
    "w_{ij}= tf_{ij} \\cdot idf_{i}.\n",
    "\\end{equation}\n",
    "A high weight $w_{ij}$ is reached by a high frequency of the term $t_{i}$ in the document $d_{j}$ and a low frequency of the term $t_{i}$ in the whole collection of documents. Hence, the weights will tend to filter out common terms that appear in many documents in the collection.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03116d0e",
   "metadata": {},
   "source": [
    "### 2.6 Boolean and Vector model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ae27c",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "The <b><i>boolean model</b></i> is a simple retrieval model based set theory on <b><i>boolean algebra</b></i>. The concept of a set is intuitive, <b><i>index terms</b></i> and queries are specified as <b><i>boolean</b></i> expressions. The <b><i>boolean model</b></i> provides easy tools that are easy to be grasped by a common user of an information retrieval system. \n",
    "The <b><i>vector model</b></i> represents documents and queries as vectors. The dimensionality of each vector is fixed, so that we can compare similarity of each document $d_{j}$ and each query $q_{l}$ based on the <b><i>cosine of the angle</b></i> between two vectors that represent them. It is expressed according to following rules\n",
    "\\begin{equation}\n",
    "sim(d_{j},q_{l})=\\frac{\\langle\\vec{d_{j}},\\vec{q_{l}}\\rangle}{||\\vec{d_{j}}||\\cdot||\\vec{q_{l}}||}.\n",
    "\\end{equation}\n",
    "\n",
    "Each dimension corresponds to a unique term chosen in advance. Its value describes the significance of the term with respect the document. In the simplest case, it can be the number of times the term occurs in the document. The <b><i>vector model</b></i> disregards the order in which the terms appear in a document.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8acf99",
   "metadata": {},
   "source": [
    "## 3. List of tasks\n",
    "<br>\n",
    "<font size=\"3\">\n",
    "\n",
    "Implement the below tasks **in sequence**. For any task that requires additional explanation, include it in your report. All six python files as detailed in section 4.1 are required to be submitted. You may submit jupyter notebooks as well. Also include a report.pdf with your submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744581c",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li> Use e.g. 15 articles from NASA corpus to obtain raw data (clean the data by removing punctuations, special characters and digits and then apply <b><i>tokenisation</i></b>).</li>\n",
    "    <li> Perform stemming using <b><i>Porter algorithm</i></b> and visualize frequency of words using the <b><i>tag clouds</i></b> for 50 most frequent words. Plot the frequency distribution for the 20 most occurent words in the first NASA article.</li>\n",
    "    <li>Compute  <b><i>term frequency</i></b> $tf_{ij}$ and <b><i>tf-idf</i></b> $w_{ij}$ for each document.\n",
    "First, choose and compare the top $p$ stems according to <b><i>term frequency</i></b> or <b><i>tf-idf</i></b> for each document. Then, compare the extracted keywords among all documents. Explain differences based on keyword criteria. </li>\n",
    "    <li>Build <b><i>boolean</i></b> and <b><i>vector models</i></b> based on top $p$ stems, then provide $k$ queries to each IR system. Compare rankings of relevant articles.</li>\n",
    "    <li>First, remove <b><i>stop words</i></b>, then perform stemming and create the <b><i>tag-clouds</i></b> again but for 50 most frequent words.</li>\n",
    "    <li>Compute again <b><i>term frequency</i></b> $tf_{ij}$ and <b><i>tf-idf</i></b> $w_{ij}$ for each document. First, choose and compare the top $p$ stems according to <b><i>term frequency</i></b> or <b><i>tf-idf</i></b> for each document. Then, compare the extracted keywords among all documents. Explain differences based on keyword criteria.\n",
    " Based on these, build new <b><i>boolean</i></b> and <b><i>vector models</i></b>. Then provide the same $k$ queries to each IR system. Compare current rankings of relevant articles with obtained before.</li>\n",
    "    <li>Based on the features you have extracted (<b><i>tf-idf matrix</i></b>), group similar NASA articles together to find some structure within them. You can apply a clustering method of your preference from <a href=\"https://scikit-learn.org/stable/modules/clustering.html\" target=\"_blank\">scikit-learn</a> and visualize the clustering output.\n",
    "</li>\n",
    "    <li>Provide some final remarks and conclusions.</li>\n",
    "</ol>\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57890888",
   "metadata": {},
   "source": [
    "## 4. Practical requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754139c3",
   "metadata": {},
   "source": [
    "### 4.1 Implementation of required functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623d5f8",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "<ul>\n",
    "    <li><b>createTermDocumentMatrix.py</b>  reads all <b> .txt</b> from a specifed directory (where your corpus is), creates a boolean or a vector representation for each document, and puts them together into a term-document matrix.</li>\n",
    "    <li><b>compQueryBoolean.py</b> compares the boolean representation of a query to the documents in the term-document matrix, and prints out similarities and filenames of the top $N$ documents.</li>\n",
    "    <li><b>compQueryVector.py</b> compares the vector representation of a query to the documents in the term-document matrix based on the cosine similarity, and prints out similarities and filenames of the top-$N$ documents.</li>\n",
    "    <li><b>similarityMeasure.py</b> nested  in <b>compQueryVector.py</b>, that measures the cosine similarity between the query and documents vector representation in collections, and returns a similarity measure of the top $N$ documents.</li>\n",
    "    <li><b>queryBooleanRepresentation.py</b> returns a boolean representation of a query.</li>\n",
    "    <li><b>queryVertorRepresentation.py</b> returns a vector representation of a query.</li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e124353",
   "metadata": {},
   "source": [
    "### 4.2 Required evaluation corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6796d",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "<b>NASA</b> collection covers 141 short articles in <b>nasa.tar.gz</b> file.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e9563",
   "metadata": {},
   "source": [
    "## 5. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8945d9",
   "metadata": {},
   "source": [
    "### 5.1 Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443effb",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "The assessment is based on your report. Your report should be submitted as a PDF document, following the requirements of a scientific report, including:\n",
    "<ul>\n",
    "    <li>a mathematical description of your solution\n",
    "    <li>description of how you went about implementing these solutions. Remeber that Information Retrieval applies to large-scale corpus with the constraint of fast responses.\n",
    "    <li>references when appropriate\n",
    " </ul>\n",
    "  \n",
    "Your PDF report should include all experimental results, your answers to all questions, and your analysis and comments of the experimental results. Please try to detail the report by giving examples and conclusions.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb558c",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Compress the 6 python files and the Report.pdf file into &lt;RollNumber&gt;_Assignment1.zip, which you should submit on Moodle.<br>\n",
    "\n",
    "An in person evaluation will be conducted, in which you are required to walk us through your code and report.<br>\n",
    "\n",
    "Please note that the deadline is **11th September 2023**, and **will not be extended.**<br><br>\n",
    "    \n",
    "**Total marks** - 25 marks. <br>\n",
    "- **Preprocessing** (3 marks)<br>\n",
    "- **TF-IDF** (5 marks)<br>\n",
    "- **Boolean model** (5 marks)<br>\n",
    "- **Vector model** (5 marks)<br>\n",
    "- **Report and explanations** (7 marks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
