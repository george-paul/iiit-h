{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3622bff",
   "metadata": {},
   "source": [
    "# Assignment 4 -  Word2Vec and DPR  (7/11/2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d366cb29",
   "metadata": {},
   "source": [
    "## 1 Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fdc87",
   "metadata": {},
   "source": [
    "### 1.1 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3e71c",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "In the first part of the exercise, we utilize Word2Vec algorithm for performing IR. Word2Vec is a state of the art algorithm to generate fixed length distributed vector representation of all the words in huge corpus. It uses fixed-dimension vectors to incorporate semantic information. Word2Vec vectors are highly efficient at grouping similar words together. It can be implemented in two architectures â€” Continuous Bag of Words(CBOW) and Skip-Gram.\n",
    "    \n",
    "<ul>\n",
    "    <li> <b>CBOW</b>: Predicts the center word based on the context (outside) words. </li>\n",
    "    <li> <b>Skip-Gram</b>: Predicts the context words based on the center word </li>\n",
    "</ul>\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e6ae2",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*_fWapW_nfSIHIXf0HaWRNw.jpeg\" width=\"500\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bed0c2f",
   "metadata": {},
   "source": [
    "### 1.2 Dense Passage Retrieval (DPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d649ab",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "In the second part of the exercise, we utilize DPR for performing IR. In dense passage retrieval, dense embeddings are used to retrieve passages for IR. The advantage of this method is that it can retrieve any passage based on semantics, which makes it more robust. \n",
    "\n",
    "DPR is a dual-encoder framework. Meaning there are two encoders present in this architecture. One encoder is used to encode a question, while the other encoder is used to encode the passages into dense embeddings. The retriever ranks and selects the most promising candidate passages by calculating the similarity between the query and the passages. This process significantly reduces the search space and improves efficiency.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015d307",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*Tze-uN_o4UdUjCg315qwzQ.png\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd5adc",
   "metadata": {},
   "source": [
    "## 2. List of tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34b7c0",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "Use the <a href=\"https://iiitaphyd-my.sharepoint.com/:u:/g/personal/ankita_maity_research_iiit_ac_in/Efxk_P-2TZhPiHddagwC1VgBA_vHE2a2k3vVCU5OF59c8g?e=XXfee9\" target=\"_blank\">NASA corpus</a> as the dataset for the tasks defined below. Further, define a set of example queries (atleast 5) relevant to the dataset for all tasks. You can take the help of '.key' files of documents to understand their keywords and compose queries / evaluate rankings.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75d8cd",
   "metadata": {},
   "source": [
    "### 2.1 Implementing Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744581c",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Your task here is to use Word2Vec to perform IR on NASA dataset.<br>\n",
    "    \n",
    "<ul>\n",
    "    <li>Use \"gensim\" package to train Word2Vec on NASA corpus. Utilize this model to obtain query and document vectors by mean-pooling query and document word-embeddings respectively. Use cosine similarity for ranking and retrieving relevant documents for above defined example queries. </li>\n",
    "    <li>Use \"spacy\" package for utilizing pre-trained word embeddings (GloVe embeddings can also be used), instead of training the word-embedding model from scratch. Subsequently, perform IR for example queries as above. </li>\n",
    "    <li>Compare the performance of retrieval in both of the above cases, with the help of examples. </li>\n",
    "</ul>\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0eeea",
   "metadata": {},
   "source": [
    "### 2.2 Implementing DPR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6ca72",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Your task here is to use DPR to perform IR on NASA dataset.<br>\n",
    "    \n",
    "<ul>\n",
    "    <li>Use DPR from the <a href=\"https://huggingface.co/docs/transformers/model_doc/dpr\" target=\"_blank\">huggingface</a> library (or any other pre-existing implementation) to rank and retrieve relevant documents for above defined example queries. </li>\n",
    "    <li>Compare the performance of retrieval in this case, with above 2 approaches and document your findings. </li>\n",
    "</ul>\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57890888",
   "metadata": {},
   "source": [
    "##  3. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443effb",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "\n",
    "The assessment is based on your code and report. Your PDF report should include all experimental results, and your analysis and comments of the experimental results. Please try to detail the report by giving examples and conclusions, and discussing different aspects such as efficiency, effectiveness of approaches, etc.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd207b4",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Compress your code - written either in python scripts, or Jupyter Notebook(s), and the Report.pdf file, into &lt;RollNumber&gt;_Assignment4.zip, which you should submit on Moodle.<br>\n",
    "\n",
    "An in person evaluation will be conducted, in which you are required to walk us through your code and report.<br>\n",
    "\n",
    "Please note that the deadline is **17th November 2023**, and **will not be extended.** Use moodle for all queries.<br>\n",
    "    \n",
    "**Total marks** - 25 marks. <br>\n",
    "- **Word2Vec implementation** (9 marks)<br>\n",
    "- **DPR implementation** (9 marks)<br>\n",
    "- **Report and explanations** (7 marks)\n",
    "    \n",
    "*Note that only best 3 out of 4 assignment scores will be considered during final grading for the course.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
